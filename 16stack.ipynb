{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Activation, Dropout, Reshape, Flatten, Permute, Lambda\n",
    "from keras.activations import softmax\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import concatenate, Concatenate, Dot\n",
    "from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D, Input, SpatialDropout1D, Bidirectional, TimeDistributed\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras import backend as K\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = 'words' # based on words or chars\n",
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 20890 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 15 # max number of words in a comment to use\n",
    "num_rnn_units = 256\n",
    "num_hidden_units = 200\n",
    "drop_prob = 0.2\n",
    "max_norm = 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = './train.csv'\n",
    "TEST_PATH = './test.csv'\n",
    "QUESTION_PATH = './question.csv'\n",
    "embed_files = {'words': './word_embed.txt', 'chars': './char_embed.txt'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get question id from a list. Remove the Q\n",
    "def get_ids(qids):\n",
    "    ids = []\n",
    "    for t_ in qids:\n",
    "        ids.append(int(t_[1:]))\n",
    "    return np.asarray(ids)\n",
    "\n",
    "# Get the text\n",
    "def get_texts(q_list, question_path=QUESTION_PATH):\n",
    "    qes = pd.read_csv(question_path)\n",
    "    ids = get_ids(q_list)\n",
    "    all_tokens = qes[token]\n",
    "    texts = [all_tokens[t] for t in ids]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_PATH)\n",
    "x_train = list(zip(train['q1'], train['q2']))\n",
    "y_train = train['label']\n",
    "\n",
    "# get the text list of question 1 and 2\n",
    "q1_train = [i[0] for i in x_train]\n",
    "text1_train = get_texts(q1_train)\n",
    "q2_train = [i[1] for i in x_train]\n",
    "text2_train = get_texts(q2_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(TEST_PATH)\n",
    "x_test = list(zip(test['q1'], test['q2']))\n",
    "\n",
    "# get the text list of question 1 and 2\n",
    "q1_test = [i[0] for i in x_test]\n",
    "text1_test = get_texts(q1_test)\n",
    "q2_test = [i[1] for i in x_test]\n",
    "text2_test = get_texts(q2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features, lower=False) # Don't lower the W or L!!!\n",
    "tokenizer.fit_on_texts(pd.read_csv(QUESTION_PATH)[token])\n",
    "\n",
    "# train set\n",
    "tokenized1_train = tokenizer.texts_to_sequences(text1_train)\n",
    "tokenized2_train = tokenizer.texts_to_sequences(text2_train)\n",
    "X1_train = pad_sequences(tokenized1_train, maxlen=maxlen)\n",
    "X2_train = pad_sequences(tokenized2_train, maxlen=maxlen)\n",
    "\n",
    "# test set\n",
    "tokenized1_test = tokenizer.texts_to_sequences(text1_test)\n",
    "tokenized2_test = tokenizer.texts_to_sequences(text2_test)\n",
    "X1_test = pad_sequences(tokenized1_test, maxlen=maxlen)\n",
    "X2_test = pad_sequences(tokenized2_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the pretrained word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20891\n"
     ]
    }
   ],
   "source": [
    "def get_coefs(line): return line[0], np.asarray(line[1:], dtype='float32')\n",
    "embed_file = embed_files[token]\n",
    "embeddings_index = dict(get_coefs(o.strip().split()) for o in open(embed_file, encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.015683081, 1.1956546)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embs = np.hstack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features+1, embed_size))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i > max_features: break\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    #print (i, word, len(embedding_vector))\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.asarray(embedding_matrix, dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_distributed(input_, layers):\n",
    "    \"Apply a list of layers in TimeDistributed mode\"\n",
    "    out_ = []\n",
    "    node_ = input_\n",
    "    for layer_ in layers:\n",
    "        node_ = TimeDistributed(layer_)(node_)\n",
    "    out_ = node_\n",
    "    return out_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_model():\n",
    "    i1 = Input(shape=(maxlen,))\n",
    "    i2 = Input(shape=(maxlen,))\n",
    "\n",
    "    # embedding layer\n",
    "    emb = Embedding(max_features+1, embed_size, weights=[embedding_matrix], trainable=False)\n",
    "    inp1 = emb(i1)\n",
    "    inp2 = emb(i2)\n",
    "\n",
    "    # enhance lstm\n",
    "    sd = SpatialDropout1D(drop_prob)\n",
    "    gru_att = Bidirectional(LSTM(num_rnn_units, return_sequences=True, dropout=drop_prob, recurrent_dropout=drop_prob))\n",
    "    inp1 = gru_att(sd(inp1))\n",
    "    inp2 = gru_att(sd(inp2))\n",
    "\n",
    "    # attend\n",
    "    da_layers = [\n",
    "        Dense(num_hidden_units, activation='relu'),\n",
    "        Dropout(drop_prob),\n",
    "        Dense(num_hidden_units, activation='relu'),\n",
    "        Dropout(drop_prob)]\n",
    "    x1 = time_distributed(inp1, da_layers)\n",
    "    x2 = time_distributed(inp2, da_layers)\n",
    "    attention = Dot(axes=-1)([x1, x2])\n",
    "    w_att_1 = Lambda(lambda x: softmax(x, axis=1))(attention)\n",
    "    w_att_2 = Permute((2,1))(Lambda(lambda x: softmax(x, axis=2))(attention))\n",
    "    x1_aligned = Dot(axes=1)([w_att_1, inp1])\n",
    "    x2_aligned = Dot(axes=1)([w_att_2, inp2])\n",
    "\n",
    "    # compose\n",
    "    x1_combined = Concatenate()([inp1, x2_aligned])\n",
    "    x2_combined = Concatenate()([inp2, x1_aligned])\n",
    "    gru_com = Bidirectional(LSTM(num_rnn_units, return_sequences=True, dropout=drop_prob, recurrent_dropout=drop_prob))\n",
    "    x1_compare = gru_com(x1_combined)\n",
    "    x2_compare = gru_com(x2_combined)\n",
    "\n",
    "    compare_layers = [\n",
    "        Dense(num_hidden_units, activation='relu'),\n",
    "        Dropout(drop_prob),\n",
    "        Dense(num_hidden_units, activation='relu'),\n",
    "        Dropout(drop_prob)]\n",
    "    x1_compare = time_distributed(x1_compare, compare_layers)\n",
    "    x2_compare = time_distributed(x2_compare, compare_layers)\n",
    "\n",
    "    # aggregate\n",
    "    gmp1 = GlobalMaxPooling1D()(x1_compare)\n",
    "    gap1 = GlobalAveragePooling1D()(x1_compare)\n",
    "    gmp2 = GlobalMaxPooling1D()(x2_compare)\n",
    "    gap2 = GlobalAveragePooling1D()(x2_compare)\n",
    "    \n",
    "    conc = concatenate([gmp1, gap1, gmp2, gap2])\n",
    "    x = BatchNormalization()(conc)\n",
    "    x = Dense(num_hidden_units, activation='relu')(x)\n",
    "    x = Dropout(drop_prob)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(num_hidden_units, activation='relu')(x)\n",
    "    x = Dropout(drop_prob)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(num_hidden_units, activation='relu')(x)\n",
    "    x = Dropout(drop_prob)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(num_hidden_units, activation='relu')(x)\n",
    "    x = Dropout(drop_prob)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=[i1, i2], outputs=x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model and predict to make the first layer feature\n",
    "Split the train set into 8 folds. Train the model using 7 folds.\n",
    "\n",
    "Predict the remaining 1 fold and concatenate. The sequence is not changed in order to unify different models.\n",
    "\n",
    "Predict the test set and take average. The sequence is unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Fold 1: ==========\n",
      "Train on 222587 samples, validate on 31799 samples\n",
      "Epoch 1/1\n",
      "222587/222587 [==============================] - 73s 326us/step - loss: 0.4612 - acc: 0.7812 - val_loss: 0.3457 - val_acc: 0.8465\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "========== Fold 2: ==========\n",
      "Train on 222587 samples, validate on 31799 samples\n",
      "Epoch 1/1\n",
      "222587/222587 [==============================] - 71s 320us/step - loss: 0.4585 - acc: 0.7840 - val_loss: 0.3422 - val_acc: 0.8478\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "========== Fold 3: ==========\n",
      "Train on 222588 samples, validate on 31798 samples\n",
      "Epoch 1/1\n",
      "222588/222588 [==============================] - 71s 320us/step - loss: 0.4622 - acc: 0.7818 - val_loss: 0.3383 - val_acc: 0.8494\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "========== Fold 4: ==========\n",
      "Train on 222588 samples, validate on 31798 samples\n",
      "Epoch 1/1\n",
      "222588/222588 [==============================] - 72s 324us/step - loss: 0.4624 - acc: 0.7809 - val_loss: 0.3449 - val_acc: 0.8468\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "========== Fold 5: ==========\n",
      "Train on 222588 samples, validate on 31798 samples\n",
      "Epoch 1/1\n",
      "222588/222588 [==============================] - 71s 321us/step - loss: 0.4660 - acc: 0.7791 - val_loss: 0.3488 - val_acc: 0.8432\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "========== Fold 6: ==========\n",
      "Train on 222588 samples, validate on 31798 samples\n",
      "Epoch 1/1\n",
      "222588/222588 [==============================] - 72s 325us/step - loss: 0.4686 - acc: 0.7784 - val_loss: 0.3513 - val_acc: 0.8467\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "========== Fold 7: ==========\n",
      "Train on 222588 samples, validate on 31798 samples\n",
      "Epoch 1/1\n",
      "222588/222588 [==============================] - 71s 321us/step - loss: 0.4635 - acc: 0.7798 - val_loss: 0.3407 - val_acc: 0.8508\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "========== Fold 8: ==========\n",
      "Train on 222588 samples, validate on 31798 samples\n",
      "Epoch 1/1\n",
      "222588/222588 [==============================] - 72s 325us/step - loss: 0.4671 - acc: 0.7780 - val_loss: 0.3388 - val_acc: 0.8505\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=8)\n",
    "n_fold = 0\n",
    "val_pred_list = []\n",
    "test_pred_list = []\n",
    "y_val_list = []\n",
    "for train_index, valid_index in kf.split(X1_train):\n",
    "    n_fold += 1\n",
    "    print (\"========== Fold %d: ==========\"%n_fold)\n",
    "    \n",
    "    # split samples\n",
    "    X1_tra, X1_val = X1_train[train_index], X1_train[valid_index]\n",
    "    X2_tra, X2_val = X2_train[train_index], X2_train[valid_index]\n",
    "    y_tra, y_val = y_train[train_index], y_train[valid_index]\n",
    "    \n",
    "    # build the model\n",
    "    #K.clear_session()\n",
    "    adam = optimizers.Adam(clipnorm=max_norm)\n",
    "    model = single_model()\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    \n",
    "    # train the model\n",
    "    #cp = ModelCheckpoint(filepath=\"my_model11-%d.h5\"%n_fold, save_best_only=True)\n",
    "    es = EarlyStopping(patience=4)\n",
    "    rp = ReduceLROnPlateau(patience = 0)\n",
    "    hist = model.fit([X1_tra, X2_tra], y_tra, batch_size = 256, epochs=20, validation_data=([X1_val, X2_val], y_val), callbacks=[es, rp])\n",
    "    \n",
    "    # load the best checkpoint and predict\n",
    "    #K.clear_session()\n",
    "    #del model\n",
    "    #model = load_model(\"my_model11-%d.h5\"%n_fold)\n",
    "    val_pred = model.predict([X1_val, X2_val], batch_size=1024)\n",
    "    val_pred_list.append(val_pred)\n",
    "    y_val_list.append(y_val)\n",
    "    test_pred = model.predict([X1_test, X2_test], batch_size=1024)\n",
    "    test_pred_list.append(test_pred)\n",
    "    del X1_tra, X1_val, X2_tra, X2_val, y_tra, y_val, model, val_pred, test_pred, hist\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred_list = np.concatenate(val_pred_list)\n",
    "test_pred_list = np.asarray(test_pred_list)\n",
    "y_val_list = np.concatenate(y_val_list)\n",
    "test_pred_list = np.mean(test_pred_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred_list = np.squeeze(val_pred_list)\n",
    "test_pred_list = np.squeeze(test_pred_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the array into csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame({'train':val_pred_list, 'label':y_val_list})\n",
    "test = pd.DataFrame({'test':test_pred_list})\n",
    "train.to_csv('train16.csv', index=False)\n",
    "test.to_csv('test16.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
