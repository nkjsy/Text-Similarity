{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'  # 安装graphviz的路径，用于模型可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, SpatialDropout1D, GRU, BatchNormalization, RepeatVector, Lambda\n",
    "from keras.layers.merge import concatenate, multiply, dot\n",
    "from keras.layers import Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D, TimeDistributed\n",
    "from keras.models import Model, Sequential\n",
    "from keras import optimizers, layers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.utils import plot_model\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = 'words' # based on words or chars\n",
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 20890 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 40 # max number of words in a comment to use\n",
    "num_rnn_units = 128 # output dim for LSTM or GRU\n",
    "num_hidden_units = 200 # output dim for dense layer\n",
    "drop_prob = 0.2\n",
    "max_norm = 5.0 # gradient clipping\n",
    "mp = 5 # multi-perspective matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = './train.csv'\n",
    "TEST_PATH = './test.csv'\n",
    "QUESTION_PATH = './question.csv'\n",
    "embed_files = {'words': './word_embed.txt', 'chars': './char_embed.txt'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get question id from a list. Remove the Q\n",
    "def get_ids(qids):\n",
    "    ids = []\n",
    "    for t_ in qids:\n",
    "        ids.append(int(t_[1:]))\n",
    "    return np.asarray(ids)\n",
    "\n",
    "# Get the text\n",
    "def get_texts(q_list, question_path=QUESTION_PATH):\n",
    "    qes = pd.read_csv(question_path)\n",
    "    ids = get_ids(q_list)\n",
    "    all_tokens = qes[token]\n",
    "    texts = [all_tokens[t] for t in ids]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data\n",
    "split some data for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJ\\AppData\\Local\\conda\\conda\\envs\\deep\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(TRAIN_PATH)\n",
    "list_train = list(zip(train['q1'], train['q2']))\n",
    "label_train = train['label']\n",
    "#print(len(list_train), len(label_train))\n",
    "\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(list_train, label_train, train_size=0.85, random_state=8, shuffle=True)\n",
    "\n",
    "# get the text list of question 1 and 2\n",
    "q1_train = [i[0] for i in X_tra]\n",
    "text1_train = get_texts(q1_train)\n",
    "q2_train = [i[1] for i in X_tra]\n",
    "text2_train = get_texts(q2_train)\n",
    "q1_val = [i[0] for i in X_val]\n",
    "text1_val = get_texts(q1_val)\n",
    "q2_val = [i[1] for i in X_val]\n",
    "text2_val = get_texts(q2_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(TEST_PATH)\n",
    "list_test = list(zip(test['q1'], test['q2']))\n",
    "\n",
    "# get the text list of question 1 and 2\n",
    "q1_test = [i[0] for i in list_test]\n",
    "text1_test = get_texts(q1_test)\n",
    "q2_test = [i[1] for i in list_test]\n",
    "text2_test = get_texts(q2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features, lower=False) # Don't lower the W or L!!!\n",
    "tokenizer.fit_on_texts(pd.read_csv(QUESTION_PATH)[token])\n",
    "\n",
    "# train set\n",
    "tokenized1_train = tokenizer.texts_to_sequences(text1_train)\n",
    "tokenized2_train = tokenizer.texts_to_sequences(text2_train)\n",
    "X1_train = pad_sequences(tokenized1_train, maxlen=maxlen)\n",
    "X2_train = pad_sequences(tokenized2_train, maxlen=maxlen)\n",
    "\n",
    "# validation set\n",
    "tokenized1_val = tokenizer.texts_to_sequences(text1_val)\n",
    "tokenized2_val = tokenizer.texts_to_sequences(text2_val)\n",
    "X1_val = pad_sequences(tokenized1_val, maxlen=maxlen)\n",
    "X2_val = pad_sequences(tokenized2_val, maxlen=maxlen)\n",
    "\n",
    "# test set\n",
    "tokenized1_test = tokenizer.texts_to_sequences(text1_test)\n",
    "tokenized2_test = tokenizer.texts_to_sequences(text2_test)\n",
    "X1_test = pad_sequences(tokenized1_test, maxlen=maxlen)\n",
    "X2_test = pad_sequences(tokenized2_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the pretrained word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20891\n"
     ]
    }
   ],
   "source": [
    "def get_coefs(line): return line[0], np.asarray(line[1:], dtype='float32')\n",
    "embed_file = embed_files[token]\n",
    "embeddings_index = dict(get_coefs(o.strip().split()) for o in open(embed_file, encoding='utf-8'))\n",
    "print (len(embeddings_index.items()))\n",
    "#print (list(embeddings_index.items())[20890])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.015683081, 1.1956546)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embs = np.hstack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "emb_mean,emb_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features+1, embed_size))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i > max_features: break\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    #print (i, word, len(embedding_vector))\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.asarray(embedding_matrix, dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### self defined multi-perspective cosine matching layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiPerspective(Layer):\n",
    "    \"\"\"Multi-perspective Matching Layer.\n",
    "    # Arguments\n",
    "        mp_dim: single forward/backward multi-perspective dimention\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mp_dim, epsilon=1e-6, **kwargs):\n",
    "        self.mp_dim = mp_dim\n",
    "        self.epsilon = 1e-6\n",
    "        self.strategy = 4\n",
    "        super(MultiPerspective, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if isinstance(input_shape, list):\n",
    "            input_shape = input_shape[0]\n",
    "        embedding_size = input_shape[-1] / 2\n",
    "        embedding_size = int(embedding_size)\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        # input_shape is bidirectional RNN input shape\n",
    "        # kernel shape (mp_dim * 2 * self.strategy, embedding_size)\n",
    "        self.kernel = self.add_weight((int(self.mp_dim),\n",
    "                                       int(embedding_size * 2 * self.strategy)),\n",
    "                                       name='kernel',\n",
    "                                       initializer='glorot_uniform',\n",
    "                                       trainable=True)\n",
    "        self.kernel_full_fw = self.kernel[:, :embedding_size]\n",
    "        self.kernel_full_bw = self.kernel[:, embedding_size: embedding_size * 2]\n",
    "        self.kernel_attentive_fw = self.kernel[:, embedding_size * 2: embedding_size * 3]\n",
    "        self.kernel_attentive_bw = self.kernel[:, embedding_size * 3: embedding_size * 4]\n",
    "        self.kernel_max_attentive_fw = self.kernel[:, embedding_size * 4: embedding_size * 5]\n",
    "        self.kernel_max_attentive_bw = self.kernel[:, embedding_size * 5: embedding_size * 6]\n",
    "        self.kernel_max_pool_fw = self.kernel[:, embedding_size * 6: embedding_size * 7]\n",
    "        self.kernel_max_pool_bw = self.kernel[:, embedding_size * 7:]\n",
    "        self.built = True\n",
    "        super(MultiPerspective, self).build(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if isinstance(input_shape, list):\n",
    "            input_shape = input_shape[0]\n",
    "        return (input_shape[0], input_shape[1], self.mp_dim * 2 * self.strategy)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'mp_dim': self.mp_dim,\n",
    "                  'epsilon': self.epsilon}\n",
    "        base_config = super(MultiPerspective, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # h1, h2: bidirectional LSTM hidden states, include forward and backward states\n",
    "        #         (batch_size, timesteps, embedding_size * 2)\n",
    "        h1 = inputs[0]\n",
    "        h2 = inputs[1]\n",
    "        embedding_size = K.int_shape(h1)[-1] / 2\n",
    "        embedding_size = int(embedding_size)\n",
    "        h1_fw = h1[:, :, :embedding_size]\n",
    "        h1_bw = h1[:, :, embedding_size:]\n",
    "        h2_fw = h2[:, :, :embedding_size]\n",
    "        h2_bw = h2[:, :, embedding_size:]\n",
    "\n",
    "        # 4 matching strategy\n",
    "        list_matching = []\n",
    "\n",
    "        # full matching ops\n",
    "        matching_fw = self._full_matching(h1_fw, h2_fw, self.kernel_full_fw)\n",
    "        matching_bw = self._full_matching(h1_bw, h2_bw, self.kernel_full_bw)\n",
    "        list_matching.extend([matching_fw, matching_bw])\n",
    "\n",
    "        # cosine matrix\n",
    "        cosine_matrix_fw = self._cosine_matrix(h1_fw, h2_fw)\n",
    "        cosine_matrix_bw = self._cosine_matrix(h1_bw, h2_bw)\n",
    "\n",
    "        # attentive matching ops\n",
    "        matching_fw = self._attentive_matching(\n",
    "            h1_fw, h2_fw, cosine_matrix_fw, self.kernel_attentive_fw)\n",
    "        matching_bw = self._attentive_matching(\n",
    "            h1_bw, h2_bw, cosine_matrix_bw, self.kernel_attentive_bw)\n",
    "        list_matching.extend([matching_fw, matching_bw])\n",
    "\n",
    "        # max attentive matching ops\n",
    "        matching_fw = self._max_attentive_matching(\n",
    "            h1_fw, h2_fw, cosine_matrix_fw, self.kernel_max_attentive_fw)\n",
    "        matching_bw = self._max_attentive_matching(\n",
    "            h1_bw, h2_bw, cosine_matrix_bw, self.kernel_max_attentive_bw)\n",
    "        list_matching.extend([matching_fw, matching_bw])\n",
    "\n",
    "        # max pooling matching ops\n",
    "        matching_fw = self._max_pooling_matching(h1_fw, h2_fw, self.kernel_max_pool_fw)\n",
    "        matching_bw = self._max_pooling_matching(h1_bw, h2_bw, self.kernel_max_pool_bw)\n",
    "        list_matching.extend([matching_fw, matching_bw])\n",
    "\n",
    "        return K.concatenate(list_matching, axis=-1)\n",
    "\n",
    "    def _cosine_similarity(self, x1, x2):\n",
    "        \"\"\"Compute cosine similarity.\n",
    "        # Arguments:\n",
    "            x1: (..., embedding_size)\n",
    "            x2: (..., embedding_size)\n",
    "        \"\"\"\n",
    "        cos = K.sum(x1 * x2, axis=-1)\n",
    "        x1_norm = K.sqrt(K.maximum(K.sum(K.square(x1), axis=-1), self.epsilon))\n",
    "        x2_norm = K.sqrt(K.maximum(K.sum(K.square(x2), axis=-1), self.epsilon))\n",
    "        cos = cos / x1_norm / x2_norm\n",
    "        return cos\n",
    "\n",
    "    def _cosine_matrix(self, x1, x2):\n",
    "        \"\"\"Cosine similarity matrix.\n",
    "        Calculate the cosine similarities between each forward (or backward)\n",
    "        contextual embedding h_i_p and every forward (or backward)\n",
    "        contextual embeddings of the other sentence\n",
    "        # Arguments\n",
    "            x1: (batch_size, x1_timesteps, embedding_size)\n",
    "            x2: (batch_size, x2_timesteps, embedding_size)\n",
    "        # Output shape\n",
    "            (batch_size, x1_timesteps, x2_timesteps)\n",
    "        \"\"\"\n",
    "        # expand h1 shape to (batch_size, x1_timesteps, 1, embedding_size)\n",
    "        x1 = K.expand_dims(x1, axis=2)\n",
    "        # expand x2 shape to (batch_size, 1, x2_timesteps, embedding_size)\n",
    "        x2 = K.expand_dims(x2, axis=1)\n",
    "        # cosine matrix (batch_size, h1_timesteps, h2_timesteps)\n",
    "        cos_matrix = self._cosine_similarity(x1, x2)\n",
    "        return cos_matrix\n",
    "\n",
    "    def _mean_attentive_vectors(self, x2, cosine_matrix):\n",
    "        \"\"\"Mean attentive vectors.\n",
    "        Calculate mean attentive vector for the entire sentence by weighted\n",
    "        summing all the contextual embeddings of the entire sentence\n",
    "        # Arguments\n",
    "            x2: sequence vectors, (batch_size, x2_timesteps, embedding_size)\n",
    "            cosine_matrix: cosine similarities matrix of x1 and x2,\n",
    "                           (batch_size, x1_timesteps, x2_timesteps)\n",
    "        # Output shape\n",
    "            (batch_size, x1_timesteps, embedding_size)\n",
    "        \"\"\"\n",
    "        # (batch_size, x1_timesteps, x2_timesteps, 1)\n",
    "        expanded_cosine_matrix = K.expand_dims(cosine_matrix, axis=-1)\n",
    "        # (batch_size, 1, x2_timesteps, embedding_size)\n",
    "        x2 = K.expand_dims(x2, axis=1)\n",
    "        # (batch_size, x1_timesteps, embedding_size)\n",
    "        weighted_sum = K.sum(expanded_cosine_matrix * x2, axis=2)\n",
    "        # (batch_size, x1_timesteps, 1)\n",
    "        sum_cosine = K.expand_dims(K.sum(cosine_matrix, axis=-1) + self.epsilon, axis=-1)\n",
    "        # (batch_size, x1_timesteps, embedding_size)\n",
    "        attentive_vector = weighted_sum / sum_cosine\n",
    "        return attentive_vector\n",
    "\n",
    "    def _max_attentive_vectors(self, x2, cosine_matrix):\n",
    "        \"\"\"Max attentive vectors.\n",
    "        Calculate max attentive vector for the entire sentence by picking\n",
    "        the contextual embedding with the highest cosine similarity\n",
    "        as the attentive vector.\n",
    "        # Arguments\n",
    "            x2: sequence vectors, (batch_size, x2_timesteps, embedding_size)\n",
    "            cosine_matrix: cosine similarities matrix of x1 and x2,\n",
    "                           (batch_size, x1_timesteps, x2_timesteps)\n",
    "        # Output shape\n",
    "            (batch_size, x1_timesteps, embedding_size)\n",
    "        \"\"\"\n",
    "        # (batch_size, x1_timesteps)\n",
    "        max_x2_step = K.argmax(cosine_matrix, axis=-1)\n",
    "\n",
    "        embedding_size = K.int_shape(x2)[-1]\n",
    "        timesteps = K.int_shape(max_x2_step)[-1]\n",
    "        if timesteps is None:\n",
    "            timesteps = K.shape(max_x2_step)[-1]\n",
    "\n",
    "        # collapse time dimension and batch dimension together\n",
    "        # collapse x2 to (batch_size * x2_timestep, embedding_size)\n",
    "        x2 = K.reshape(x2, (-1, embedding_size))\n",
    "        # collapse max_x2_step to (batch_size * h1_timesteps)\n",
    "        max_x2_step = K.reshape(max_x2_step, (-1,))\n",
    "        # (batch_size * x1_timesteps, embedding_size)\n",
    "        max_x2 = K.gather(x2, max_x2_step)\n",
    "        # reshape max_x2, (batch_size, x1_timesteps, embedding_size)\n",
    "        attentive_vector = K.reshape(max_x2, K.stack([-1, timesteps, embedding_size]))\n",
    "        return attentive_vector\n",
    "\n",
    "    def _time_distributed_multiply(self, x, w):\n",
    "        \"\"\"Element-wise multiply vector and weights.\n",
    "        # Arguments\n",
    "            x: sequence of hidden states, (batch_size, ?, embedding_size)\n",
    "            w: weights of one matching strategy of one direction,\n",
    "               (mp_dim, embedding_size)\n",
    "        # Output shape\n",
    "            (?, mp_dim, embedding_size)\n",
    "        \"\"\"\n",
    "        # dimension of vector\n",
    "        n_dim = K.ndim(x)\n",
    "        embedding_size = K.int_shape(x)[-1]\n",
    "        timesteps = K.int_shape(x)[1]\n",
    "        if timesteps is None:\n",
    "            timesteps = K.shape(x)[1]\n",
    "\n",
    "        # collapse time dimension and batch dimension together\n",
    "        x = K.reshape(x, (-1, embedding_size))\n",
    "        # reshape to (?, 1, embedding_size)\n",
    "        x = K.expand_dims(x, axis=1)\n",
    "        # reshape weights to (1, mp_dim, embedding_size)\n",
    "        w = K.expand_dims(w, axis=0)\n",
    "        # element-wise multiply\n",
    "        x = x * w\n",
    "        # reshape to original shape\n",
    "        if n_dim == 3:\n",
    "            x = K.reshape(x, K.stack([-1, timesteps, self.mp_dim, embedding_size]))\n",
    "            x.set_shape([None, None, None, embedding_size])\n",
    "        elif n_dim == 2:\n",
    "            x = K.reshape(x, K.stack([-1, self.mp_dim, embedding_size]))\n",
    "            x.set_shape([None, None, embedding_size])\n",
    "        return x\n",
    "\n",
    "    def _full_matching(self, h1, h2, w):\n",
    "        \"\"\"Full matching operation.\n",
    "        # Arguments\n",
    "            h1: (batch_size, h1_timesteps, embedding_size)\n",
    "            h2: (batch_size, h2_timesteps, embedding_size)\n",
    "            w: weights of one direction, (mp_dim, embedding_size)\n",
    "        # Output shape\n",
    "            (batch_size, h1_timesteps, mp_dim)\n",
    "        \"\"\"\n",
    "        # h2 forward last step hidden vector, (batch_size, embedding_size)\n",
    "        h2_last_state = h2[:, -1, :]\n",
    "        # h1 * weights, (batch_size, h1_timesteps, mp_dim, embedding_size)\n",
    "        h1 = self._time_distributed_multiply(h1, w)\n",
    "        # h2_last_state * weights, (batch_size, mp_dim, embedding_size)\n",
    "        h2 = self._time_distributed_multiply(h2_last_state, w)\n",
    "        # reshape to (batch_size, 1, mp_dim, embedding_size)\n",
    "        h2 = K.expand_dims(h2, axis=1)\n",
    "        # matching vector, (batch_size, h1_timesteps, mp_dim)\n",
    "        matching = self._cosine_similarity(h1, h2)\n",
    "        return matching\n",
    "\n",
    "    def _max_pooling_matching(self, h1, h2, w):\n",
    "        \"\"\"Max pooling matching operation.\n",
    "        # Arguments\n",
    "            h1: (batch_size, h1_timesteps, embedding_size)\n",
    "            h2: (batch_size, h2_timesteps, embedding_size)\n",
    "            w: weights of one direction, (mp_dim, embedding_size)\n",
    "        # Output shape\n",
    "            (batch_size, h1_timesteps, mp_dim)\n",
    "        \"\"\"\n",
    "        # h1 * weights, (batch_size, h1_timesteps, mp_dim, embedding_size)\n",
    "        h1 = self._time_distributed_multiply(h1, w)\n",
    "        # h2 * weights, (batch_size, h2_timesteps, mp_dim, embedding_size)\n",
    "        h2 = self._time_distributed_multiply(h2, w)\n",
    "        # reshape v1 to (batch_size, h1_timesteps, 1, mp_dim, embedding_size)\n",
    "        h1 = K.expand_dims(h1, axis=2)\n",
    "        # reshape v1 to (batch_size, 1, h2_timesteps, mp_dim, embedding_size)\n",
    "        h2 = K.expand_dims(h2, axis=1)\n",
    "        # cosine similarity, (batch_size, h1_timesteps, h2_timesteps, mp_dim)\n",
    "        cos = self._cosine_similarity(h1, h2)\n",
    "        # (batch_size, h1_timesteps, mp_dim)\n",
    "        matching = K.max(cos, axis=2)\n",
    "        return matching\n",
    "\n",
    "    def _attentive_matching(self, h1, h2, cosine_matrix, w):\n",
    "        \"\"\"Attentive matching operation.\n",
    "        # Arguments\n",
    "            h1: (batch_size, h1_timesteps, embedding_size)\n",
    "            h2: (batch_size, h2_timesteps, embedding_size)\n",
    "            cosine_matrix: weights of hidden state h2,\n",
    "                          (batch_size, h1_timesteps, h2_timesteps)\n",
    "            w: weights of one direction, (mp_dim, embedding_size)\n",
    "        # Output shape\n",
    "            (batch_size, h1_timesteps, mp_dim)\n",
    "        \"\"\"\n",
    "        # h1 * weights, (batch_size, h1_timesteps, mp_dim, embedding_size)\n",
    "        h1 = self._time_distributed_multiply(h1, w)\n",
    "        # attentive vector (batch_size, h1_timesteps, embedding_szie)\n",
    "        attentive_vec = self._mean_attentive_vectors(h2, cosine_matrix)\n",
    "        # attentive_vec * weights, (batch_size, h1_timesteps, mp_dim, embedding_size)\n",
    "        attentive_vec = self._time_distributed_multiply(attentive_vec, w)\n",
    "        # matching vector, (batch_size, h1_timesteps, mp_dim)\n",
    "        matching = self._cosine_similarity(h1, attentive_vec)\n",
    "        return matching\n",
    "\n",
    "    def _max_attentive_matching(self, h1, h2, cosine_matrix, w):\n",
    "        \"\"\"Max attentive matching operation.\n",
    "        # Arguments\n",
    "            h1: (batch_size, h1_timesteps, embedding_size)\n",
    "            h2: (batch_size, h2_timesteps, embedding_size)\n",
    "            cosine_matrix: weights of hidden state h2,\n",
    "                          (batch_size, h1_timesteps, h2_timesteps)\n",
    "            w: weights of one direction, (mp_dim, embedding_size)\n",
    "        # Output shape\n",
    "            (batch_size, h1_timesteps, mp_dim)\n",
    "        \"\"\"\n",
    "        # h1 * weights, (batch_size, h1_timesteps, mp_dim, embedding_size)\n",
    "        h1 = self._time_distributed_multiply(h1, w)\n",
    "        # max attentive vector (batch_size, h1_timesteps, embedding_szie)\n",
    "        max_attentive_vec = self._max_attentive_vectors(h2, cosine_matrix)\n",
    "        # max_attentive_vec * weights, (batch_size, h1_timesteps, mp_dim, embedding_size)\n",
    "        max_attentive_vec = self._time_distributed_multiply(max_attentive_vec, w)\n",
    "        # matching vector, (batch_size, h1_timesteps, mp_dim)\n",
    "        matching = self._cosine_similarity(h1, max_attentive_vec)\n",
    "        return matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 40, 300)      6267300     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 40, 256)      439296      embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multi_perspective_1 (MultiPersp (None, 40, 40)       5120        bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_1[1][0]            \n",
      "                                                                 bidirectional_1[1][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 256)          173056      multi_perspective_1[0][0]        \n",
      "                                                                 multi_perspective_1[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           bidirectional_2[0][0]            \n",
      "                                                                 bidirectional_2[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 200)          102600      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 200)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 200)          800         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 200)          40200       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 200)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 200)          800         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            201         batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 7,029,373\n",
      "Trainable params: 761,273\n",
      "Non-trainable params: 6,268,100\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp1 = Input(shape=(maxlen,))\n",
    "inp2 = Input(shape=(maxlen,))\n",
    "\n",
    "# Word representation Layer\n",
    "\n",
    "embed = Embedding(max_features+1, embed_size, weights=[embedding_matrix], trainable = False, mask_zero=False)\n",
    "x1 = embed(inp1)\n",
    "x2 = embed(inp2)\n",
    "\n",
    "# Context representation layer\n",
    "bilstm1 = Bidirectional(LSTM(num_rnn_units, return_sequences=True, dropout=drop_prob, recurrent_dropout=drop_prob))\n",
    "x1 = bilstm1(x1) \n",
    "x2 = bilstm1(x2)\n",
    "\n",
    "# Matching layer\n",
    "match = MultiPerspective(mp)\n",
    "m1 = match([x1,x2])\n",
    "m2 = match([x2,x1])\n",
    "\n",
    "# Aggregation layer\n",
    "bilstm2 = Bidirectional(LSTM(num_rnn_units, return_sequences=False, dropout=drop_prob, recurrent_dropout=drop_prob))\n",
    "x1 = bilstm2(m1) \n",
    "x2 = bilstm2(m2)\n",
    "conc = concatenate([x1,x2])\n",
    "\n",
    "# Prediction layer\n",
    "x = Dense(num_hidden_units, activation='relu')(conc)\n",
    "x = Dropout(drop_prob)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(num_hidden_units, activation='relu')(x)\n",
    "x = Dropout(drop_prob)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs=[inp1, inp2], outputs=x)\n",
    "adam = optimizers.Adam(clipnorm=max_norm)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model.summary()\n",
    "plot_model(model, to_file='model4.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJ\\AppData\\Local\\conda\\conda\\envs\\deep\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 216228 samples, validate on 38158 samples\n",
      "Epoch 1/15\n",
      "216228/216228 [==============================] - 984s 5ms/step - loss: 0.3715 - acc: 0.8353 - val_loss: 0.3039 - val_acc: 0.8666\n",
      "Epoch 2/15\n",
      "216228/216228 [==============================] - 966s 4ms/step - loss: 0.2904 - acc: 0.8754 - val_loss: 0.2901 - val_acc: 0.8747\n",
      "Epoch 3/15\n",
      "216228/216228 [==============================] - 3623s 17ms/step - loss: 0.2666 - acc: 0.8863 - val_loss: 0.2709 - val_acc: 0.8853\n",
      "Epoch 4/15\n",
      "216228/216228 [==============================] - 1038s 5ms/step - loss: 0.2499 - acc: 0.8937 - val_loss: 0.2547 - val_acc: 0.8937\n",
      "Epoch 5/15\n",
      "216228/216228 [==============================] - 1065s 5ms/step - loss: 0.2396 - acc: 0.8978 - val_loss: 0.2450 - val_acc: 0.8975\n",
      "Epoch 6/15\n",
      "216228/216228 [==============================] - 1069s 5ms/step - loss: 0.2296 - acc: 0.9026 - val_loss: 0.2413 - val_acc: 0.8985\n",
      "Epoch 7/15\n",
      "216228/216228 [==============================] - 1065s 5ms/step - loss: 0.2230 - acc: 0.9064 - val_loss: 0.2386 - val_acc: 0.8999\n",
      "Epoch 8/15\n",
      "216228/216228 [==============================] - 1064s 5ms/step - loss: 0.2164 - acc: 0.9088 - val_loss: 0.2387 - val_acc: 0.9011\n",
      "Epoch 9/15\n",
      "216228/216228 [==============================] - 1063s 5ms/step - loss: 0.1988 - acc: 0.9167 - val_loss: 0.2405 - val_acc: 0.9042\n"
     ]
    }
   ],
   "source": [
    "cp = ModelCheckpoint(filepath=\"my_model4.h5\", save_best_only=True)\n",
    "es = EarlyStopping(patience=2)\n",
    "rp = ReduceLROnPlateau(patience=0)\n",
    "hist = model.fit([X1_train, X2_train], y_tra, batch_size = 128, epochs=15, validation_data=([X1_val, X2_val], y_val), callbacks=[cp, es, rp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': [0.30392738577704603, 0.29011233089804817, 0.2709317498408534, 0.2547201967425576, 0.2449715483725349, 0.24134434528511783, 0.23857451016013556, 0.23865854552697008, 0.24045034307835955], 'val_acc': [0.8665810577136752, 0.8746527595848425, 0.8853189370574563, 0.893731327643965, 0.8975051103369783, 0.8985009696462494, 0.8998899313318722, 0.9010954452601924, 0.9041878505100263], 'loss': [0.3715042048168625, 0.2904067356957777, 0.2666233791473966, 0.2499384963589825, 0.23957287820098222, 0.22964877146543003, 0.22302685592248495, 0.21642818976819614, 0.19883299607368074], 'acc': [0.8353127254520522, 0.8753537932193722, 0.8862913221262522, 0.8936677951040641, 0.8977976950278264, 0.9026444308829277, 0.9064228499535748, 0.9088415931287638, 0.9167082893971251], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.000100000005]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c39ef6ac88>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c1dbe98cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (hist.history)\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(1)\n",
    "plt.plot (hist.history['loss'])\n",
    "plt.plot (hist.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown layer: MultiPerspective",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-8c9be7874333>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'my_model4.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\deep\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    259\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No model found in config file.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[0mmodel_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_from_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m         \u001b[1;31m# set weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\deep\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m    333\u001b[0m                         '`Sequential.from_config(config)`?')\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\deep\\lib\\site-packages\\keras\\layers\\__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m     53\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\deep\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    143\u001b[0m                     \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'config'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                     custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n\u001b[1;32m--> 145\u001b[1;33m                                         list(custom_objects.items())))\n\u001b[0m\u001b[0;32m    146\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'config'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\deep\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mfrom_config\u001b[1;34m(cls, config, custom_objects)\u001b[0m\n\u001b[0;32m   1034\u001b[0m         \u001b[1;31m# First, we create all layers and enqueue nodes to be processed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer_data\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'layers'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1036\u001b[1;33m             \u001b[0mprocess_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1037\u001b[0m         \u001b[1;31m# Then we process nodes in order of layer depth.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m         \u001b[1;31m# Nodes that cannot yet be processed (if the inbound node\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\deep\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mprocess_layer\u001b[1;34m(layer_data)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m             layer = deserialize_layer(layer_data,\n\u001b[1;32m-> 1022\u001b[1;33m                                       custom_objects=custom_objects)\n\u001b[0m\u001b[0;32m   1023\u001b[0m             \u001b[0mcreated_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\deep\\lib\\site-packages\\keras\\layers\\__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m     53\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\deep\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    136\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                 raise ValueError('Unknown ' + printable_module_name +\n\u001b[1;32m--> 138\u001b[1;33m                                  ': ' + class_name)\n\u001b[0m\u001b[0;32m    139\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'from_config'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[0mcustom_objects\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcustom_objects\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown layer: MultiPerspective"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('my_model4.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict([X1_test, X2_test], batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make a submission file \n",
    "def make_submission(predict_prob):\n",
    "    with open('sub4.csv', 'w') as file:\n",
    "        file.write(str('y_pre') + '\\n')\n",
    "        for line in predict_prob:\n",
    "            #line = np.clip(line, 0.005, 0.995)\n",
    "            file.write(str(line[0]) + '\\n')\n",
    "    file.close()\n",
    "    \n",
    "make_submission(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
