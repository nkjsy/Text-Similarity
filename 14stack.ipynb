{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Activation, Dropout, Reshape, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import concatenate, Concatenate\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D, Input, SpatialDropout1D, Bidirectional\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras import backend as K\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras import optimizers, initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = 'words' # based on words or chars\n",
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 20890 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 15 # max number of words in a comment to use\n",
    "num_rnn_units = 128\n",
    "num_hidden_units = 300\n",
    "drop_prob = 0.1\n",
    "max_norm = 5.0\n",
    "filter_sizes = [1,2,3,5]\n",
    "num_filters = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = './train.csv'\n",
    "TEST_PATH = './test.csv'\n",
    "QUESTION_PATH = './question.csv'\n",
    "embed_files = {'words': './word_embed.txt', 'chars': './char_embed.txt'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get question id from a list. Remove the Q\n",
    "def get_ids(qids):\n",
    "    ids = []\n",
    "    for t_ in qids:\n",
    "        ids.append(int(t_[1:]))\n",
    "    return np.asarray(ids)\n",
    "\n",
    "# Get the text\n",
    "def get_texts(q_list, question_path=QUESTION_PATH):\n",
    "    qes = pd.read_csv(question_path)\n",
    "    ids = get_ids(q_list)\n",
    "    all_tokens = qes[token]\n",
    "    texts = [all_tokens[t] for t in ids]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_PATH)\n",
    "x_train = list(zip(train['q1'], train['q2']))\n",
    "y_train = train['label']\n",
    "\n",
    "# get the text list of question 1 and 2\n",
    "q1_train = [i[0] for i in x_train]\n",
    "text1_train = get_texts(q1_train)\n",
    "q2_train = [i[1] for i in x_train]\n",
    "text2_train = get_texts(q2_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(TEST_PATH)\n",
    "x_test = list(zip(test['q1'], test['q2']))\n",
    "\n",
    "# get the text list of question 1 and 2\n",
    "q1_test = [i[0] for i in x_test]\n",
    "text1_test = get_texts(q1_test)\n",
    "q2_test = [i[1] for i in x_test]\n",
    "text2_test = get_texts(q2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features, lower=False) # Don't lower the W or L!!!\n",
    "tokenizer.fit_on_texts(pd.read_csv(QUESTION_PATH)[token])\n",
    "\n",
    "# train set\n",
    "tokenized1_train = tokenizer.texts_to_sequences(text1_train)\n",
    "tokenized2_train = tokenizer.texts_to_sequences(text2_train)\n",
    "X1_train = pad_sequences(tokenized1_train, maxlen=maxlen)\n",
    "X2_train = pad_sequences(tokenized2_train, maxlen=maxlen)\n",
    "\n",
    "# test set\n",
    "tokenized1_test = tokenizer.texts_to_sequences(text1_test)\n",
    "tokenized2_test = tokenizer.texts_to_sequences(text2_test)\n",
    "X1_test = pad_sequences(tokenized1_test, maxlen=maxlen)\n",
    "X2_test = pad_sequences(tokenized2_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the pretrained word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20891\n"
     ]
    }
   ],
   "source": [
    "def get_coefs(line): return line[0], np.asarray(line[1:], dtype='float32')\n",
    "embed_file = embed_files[token]\n",
    "embeddings_index = dict(get_coefs(o.strip().split()) for o in open(embed_file, encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.015683081, 1.1956546)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embs = np.hstack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features+1, embed_size))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i > max_features: break\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    #print (i, word, len(embedding_vector))\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.asarray(embedding_matrix, dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attetion layer\n",
    "class FeedForwardAttention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "        \"\"\"\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(FeedForwardAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, x):\n",
    "\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.softmax(eij)\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #return input_shape[0], input_shape[-1]\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_model():\n",
    "    inp1 = Input(shape=(maxlen,))\n",
    "    inp2 = Input(shape=(maxlen,))\n",
    "\n",
    "    # basic rnn+cnn+attention model\n",
    "    inp = Input(shape=(maxlen,))\n",
    "    h = Embedding(max_features+1, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    h = SpatialDropout1D(drop_prob)(h)\n",
    "    h = Bidirectional(LSTM(num_rnn_units, return_sequences=True, dropout=drop_prob, recurrent_dropout=drop_prob))(h)\n",
    "    \n",
    "    conv_0 = Conv1D(num_filters, kernel_size=filter_sizes[0], padding = \"same\", activation = 'relu')(h)\n",
    "    conv_1 = Conv1D(num_filters, kernel_size=filter_sizes[1], padding = \"same\", activation = 'relu')(h)\n",
    "    conv_2 = Conv1D(num_filters, kernel_size=filter_sizes[2], padding = \"same\", activation = 'relu')(h)\n",
    "    conv_3 = Conv1D(num_filters, kernel_size=filter_sizes[3], padding = \"same\", activation = 'relu')(h)\n",
    "\n",
    "    maxpool_0 = GlobalMaxPooling1D()(conv_0)\n",
    "    avgpool_0 = GlobalAveragePooling1D()(conv_0)\n",
    "    att_0 = FeedForwardAttention(maxlen)(conv_0)\n",
    "    maxpool_1 = GlobalMaxPooling1D()(conv_1)\n",
    "    avgpool_1 = GlobalAveragePooling1D()(conv_1)\n",
    "    att_1 = FeedForwardAttention(maxlen)(conv_1)\n",
    "    maxpool_2 = GlobalMaxPooling1D()(conv_2)\n",
    "    avgpool_2 = GlobalAveragePooling1D()(conv_2)\n",
    "    att_2 = FeedForwardAttention(maxlen)(conv_2)\n",
    "    maxpool_3 = GlobalMaxPooling1D()(conv_3)\n",
    "    avgpool_3 = GlobalAveragePooling1D()(conv_3)\n",
    "    att_3 = FeedForwardAttention(maxlen)(conv_3)\n",
    "\n",
    "    z = concatenate([maxpool_0, maxpool_1, maxpool_2, maxpool_3, avgpool_0, avgpool_1, avgpool_2, avgpool_3, att_0, att_1, att_2, att_3])\n",
    "\n",
    "    base_model = Model(inputs=inp, outputs=z)\n",
    "\n",
    "    o1 = base_model(inp1)\n",
    "    o2 = base_model(inp2)\n",
    "\n",
    "    conc = concatenate([o1,o2])\n",
    "    x = BatchNormalization()(conc)\n",
    "\n",
    "    x = Dense(num_hidden_units, activation='relu')(x)\n",
    "    x = Dropout(drop_prob)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=[inp1, inp2], outputs=x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model and predict to make the first layer feature\n",
    "Split the train set into 8 folds. Train the model using 7 folds.\n",
    "\n",
    "Predict the remaining 1 fold and concatenate. The sequence is not changed in order to unify different models.\n",
    "\n",
    "Predict the test set and take average. The sequence is unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Fold 1: ==========\n",
      "Train on 222587 samples, validate on 31799 samples\n",
      "Epoch 1/1\n",
      "222587/222587 [==============================] - 73s 326us/step - loss: 0.4612 - acc: 0.7812 - val_loss: 0.3457 - val_acc: 0.8465\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "========== Fold 2: ==========\n",
      "Train on 222587 samples, validate on 31799 samples\n",
      "Epoch 1/1\n",
      "222587/222587 [==============================] - 71s 320us/step - loss: 0.4585 - acc: 0.7840 - val_loss: 0.3422 - val_acc: 0.8478\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "========== Fold 3: ==========\n",
      "Train on 222588 samples, validate on 31798 samples\n",
      "Epoch 1/1\n",
      "222588/222588 [==============================] - 71s 320us/step - loss: 0.4622 - acc: 0.7818 - val_loss: 0.3383 - val_acc: 0.8494\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "========== Fold 4: ==========\n",
      "Train on 222588 samples, validate on 31798 samples\n",
      "Epoch 1/1\n",
      "222588/222588 [==============================] - 72s 324us/step - loss: 0.4624 - acc: 0.7809 - val_loss: 0.3449 - val_acc: 0.8468\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "========== Fold 5: ==========\n",
      "Train on 222588 samples, validate on 31798 samples\n",
      "Epoch 1/1\n",
      "222588/222588 [==============================] - 71s 321us/step - loss: 0.4660 - acc: 0.7791 - val_loss: 0.3488 - val_acc: 0.8432\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "========== Fold 6: ==========\n",
      "Train on 222588 samples, validate on 31798 samples\n",
      "Epoch 1/1\n",
      "222588/222588 [==============================] - 72s 325us/step - loss: 0.4686 - acc: 0.7784 - val_loss: 0.3513 - val_acc: 0.8467\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "========== Fold 7: ==========\n",
      "Train on 222588 samples, validate on 31798 samples\n",
      "Epoch 1/1\n",
      "222588/222588 [==============================] - 71s 321us/step - loss: 0.4635 - acc: 0.7798 - val_loss: 0.3407 - val_acc: 0.8508\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "========== Fold 8: ==========\n",
      "Train on 222588 samples, validate on 31798 samples\n",
      "Epoch 1/1\n",
      "222588/222588 [==============================] - 72s 325us/step - loss: 0.4671 - acc: 0.7780 - val_loss: 0.3388 - val_acc: 0.8505\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=8)\n",
    "n_fold = 0\n",
    "val_pred_list = []\n",
    "test_pred_list = []\n",
    "y_val_list = []\n",
    "for train_index, valid_index in kf.split(X1_train):\n",
    "    n_fold += 1\n",
    "    print (\"========== Fold %d: ==========\"%n_fold)\n",
    "    \n",
    "    # split samples\n",
    "    X1_tra, X1_val = X1_train[train_index], X1_train[valid_index]\n",
    "    X2_tra, X2_val = X2_train[train_index], X2_train[valid_index]\n",
    "    y_tra, y_val = y_train[train_index], y_train[valid_index]\n",
    "    \n",
    "    # build the model\n",
    "    #K.clear_session()\n",
    "    adam = optimizers.Adam(clipnorm=max_norm)\n",
    "    model = single_model()\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    \n",
    "    # train the model\n",
    "    #cp = ModelCheckpoint(filepath=\"my_model11-%d.h5\"%n_fold, save_best_only=True)\n",
    "    es = EarlyStopping(patience=4)\n",
    "    rp = ReduceLROnPlateau(patience = 0)\n",
    "    hist = model.fit([X1_tra, X2_tra], y_tra, batch_size = 256, epochs=20, validation_data=([X1_val, X2_val], y_val), callbacks=[es, rp])\n",
    "    \n",
    "    # load the best checkpoint and predict\n",
    "    #K.clear_session()\n",
    "    #del model\n",
    "    #model = load_model(\"my_model11-%d.h5\"%n_fold)\n",
    "    val_pred = model.predict([X1_val, X2_val], batch_size=1024)\n",
    "    val_pred_list.append(val_pred)\n",
    "    y_val_list.append(y_val)\n",
    "    test_pred = model.predict([X1_test, X2_test], batch_size=1024)\n",
    "    test_pred_list.append(test_pred)\n",
    "    del X1_tra, X1_val, X2_tra, X2_val, y_tra, y_val, model, val_pred, test_pred, hist\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred_list = np.concatenate(val_pred_list)\n",
    "test_pred_list = np.asarray(test_pred_list)\n",
    "y_val_list = np.concatenate(y_val_list)\n",
    "test_pred_list = np.mean(test_pred_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred_list = np.squeeze(val_pred_list)\n",
    "test_pred_list = np.squeeze(test_pred_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the array into csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame({'train':val_pred_list, 'label':y_val_list})\n",
    "test = pd.DataFrame({'test':test_pred_list})\n",
    "train.to_csv('train14.csv', index=False)\n",
    "test.to_csv('test14.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
